{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The spectral-data-converter library (and its dependent libraries) can be used for converting spectral data from one format into another. Please refer to the dataset formats section for more details on supported formats. But the library does not just convert data formats, you can also slot in complex filter pipelines to process/clean the data. On this website you can find examples and documentation for: Filter usage Placeholders Execution control External functions Multiple I/O ZIP files Direct I/O Email File handling Temporary storage Docker usage Examples for the additional libraries: scikit-learn Visualization","title":"Home"},{"location":"directio/","text":"The spectral-data-converter library is mainly designed to be used as a command-line pipeline for working with files. However, you can also use any of the readers/writers that implement the seppl.io.DirectReader / seppl.io.DirectWriter mixins. By being able to handle file-like objects, you can move away from just using files via paths. Though the examples below are still using files for reading/writing, the concept is the same if you want to access other types of data. Spectra # The following code snippet reads an SPA file and writes the spectra in ADAMS format to another directory: from sdc.reader import SPAReader as Reader from sdc.writer import AdamsWriter as Writer from seppl import Session r = Reader(direct_read=True) r.initialize() r.session = Session() w = Writer(output_sampledata=True) w.initialize() w.session = r.session f_in = \"./input/cylohex.spa\" with open(f_in, \"rb\") as fp: sps = [x for x in r.read_fp(fp)] f_out = \"./output/\" for i, sp in enumerate(sps): with open(f_out + str(i) + \".spec\", \"w\") as fp: w.write_stream_fp(sp, fp, False) Sample data # The following code snippet reads sample data records from a CSV file and writes them in JSON format to another directory: from sdc.reader import CSVSampleDataReader as Reader from sdc.writer import JsonSampleDataWriter as Writer from seppl import Session r = Reader(direct_read=True, sample_id=\"1\", sample_data=\"2-last\") r.initialize() r.session = Session() w = Writer() w.initialize() w.session = r.session f_in = \"./input/sampledata.csv\" with open(f_in, \"r\") as fp: sds = [x for x in r.read_fp(fp)] f_out = \"./output/\" for i, sd in enumerate(sds): with open(f_out + str(i) + \".json\", \"w\") as fp: w.write_stream_fp(sd, fp, False)","title":"Direct I/O"},{"location":"directio/#spectra","text":"The following code snippet reads an SPA file and writes the spectra in ADAMS format to another directory: from sdc.reader import SPAReader as Reader from sdc.writer import AdamsWriter as Writer from seppl import Session r = Reader(direct_read=True) r.initialize() r.session = Session() w = Writer(output_sampledata=True) w.initialize() w.session = r.session f_in = \"./input/cylohex.spa\" with open(f_in, \"rb\") as fp: sps = [x for x in r.read_fp(fp)] f_out = \"./output/\" for i, sp in enumerate(sps): with open(f_out + str(i) + \".spec\", \"w\") as fp: w.write_stream_fp(sp, fp, False)","title":"Spectra"},{"location":"directio/#sample-data","text":"The following code snippet reads sample data records from a CSV file and writes them in JSON format to another directory: from sdc.reader import CSVSampleDataReader as Reader from sdc.writer import JsonSampleDataWriter as Writer from seppl import Session r = Reader(direct_read=True, sample_id=\"1\", sample_data=\"2-last\") r.initialize() r.session = Session() w = Writer() w.initialize() w.session = r.session f_in = \"./input/sampledata.csv\" with open(f_in, \"r\") as fp: sds = [x for x in r.read_fp(fp)] f_out = \"./output/\" for i, sd in enumerate(sds): with open(f_out + str(i) + \".json\", \"w\") as fp: w.write_stream_fp(sd, fp, False)","title":"Sample data"},{"location":"docker/","text":"Below are examples for using the spectral-data-converter library via its Docker images . Interactive session # The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/spectral-data-converter:latest Conversion pipeline # The following converts an spectra in ADAMS format to ASC format: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/spectral-data-converter:latest \\ sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i /workspace/input/*.spec \\ to-asc \\ -l INFO \\ -o /workspace/output NB: The input and output directories are located below the current working directory ( pwd ).","title":"Docker usage"},{"location":"docker/#interactive-session","text":"The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/spectral-data-converter:latest","title":"Interactive session"},{"location":"docker/#conversion-pipeline","text":"The following converts an spectra in ADAMS format to ASC format: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/spectral-data-converter:latest \\ sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i /workspace/input/*.spec \\ to-asc \\ -l INFO \\ -o /workspace/output NB: The input and output directories are located below the current working directory ( pwd ).","title":"Conversion pipeline"},{"location":"email/","text":"Whilst most readers and writers are file-based, it is also possible to retrieve and send emails using the following: get-email - retrieve emails from an IMAP folder, FROM and SUBJECT can be stored in placeholders send-email - send emails via SMTP, the from/to/subject/body options automatically expand placeholders The connection parameters are obtained through environment variables that are stored in .env files. The following pipeline uses the get-email reader to poll the spectra folder for new messages with .spec attachments every 5 seconds. These spectra get downloaded and then forwarded to the send-email writer that attaches the spectra and then sends an email to the specified email address: sdc-convert -l INFO \\ get-email \\ -l INFO \\ -f spectra \\ -o {TMP} \\ -r \".*\\.spec\" \\ -w 5 \\ --only_unseen \\ --mark_as_read \\ --from_placeholder FROM \\ --subject_placeholder SUBJECT \\ send-email \\ -l INFO \\ -f from@example.com \\ -t someone@anotherexample.com \\ -s {SUBJECT} \\ -b \"Message from: {FROM}\"","title":"Email"},{"location":"execution_control/","text":"The following filters can be used for controlling the execution of the pipeline: block - blocks data passing through based on a condition applied to the meta-data list-to-sequence - forwards the elements of lists individually stop - stops the pipeline if the meta-data-based condition holds true sub-process - meta-data condition determines execution of sub-filter(s) tee - meta-data condition determines forking off of data to the sub-pipeline (filter(s), [writer]) trigger - meta-data condition determines execution of the sub-pipeline (reader, [filter(s)], [writer]) Sub-pipelines # With the tee meta-filter, it is possible to filter the spectra coming through with a separate sub-pipeline. That allows converting the incoming data into multiple output formats with their own preprocessing. The following command loads the ADAMS annotations and saves them in ASC and ASCII XY format in one command, but one with centered and the other with log-transformed data: sdc-convert \\ -l INFO \\ -b \\ from-adams \\ -l INFO \\ -i \"./adams/*.spec\" \\ tee \\ -f \"center to-asc -l INFO -o ./tee-asc/\" \\ tee \\ -f \"standardize to-asciixy -l INFO -o ./tee-asciixy/\"","title":"Execution control"},{"location":"execution_control/#sub-pipelines","text":"With the tee meta-filter, it is possible to filter the spectra coming through with a separate sub-pipeline. That allows converting the incoming data into multiple output formats with their own preprocessing. The following command loads the ADAMS annotations and saves them in ASC and ASCII XY format in one command, but one with centered and the other with log-transformed data: sdc-convert \\ -l INFO \\ -b \\ from-adams \\ -l INFO \\ -i \"./adams/*.spec\" \\ tee \\ -f \"center to-asc -l INFO -o ./tee-asc/\" \\ tee \\ -f \"standardize to-asciixy -l INFO -o ./tee-asciixy/\"","title":"Sub-pipelines"},{"location":"file_handling/","text":"Reading/writing text files # from-text-file - this reader reads the file line by line and forwards them to-text-file - writes the incoming strings to the specified text file Listing files # list-files - simply lists files in a directory forwards the list poll-dir - polls a directory for files to process, can move or delete them after they were processed by the specified base-reader watch-dir - uses a file-system watchdog to look for changes to files (events: created or modified) and forwards these to the base-reader, can move or delete these files after processing as well Others # copy-files - copies the incoming files into the specified target directory delete-files - deletes the incoming files move-files - moves the incoming files into the specified target directory","title":"File handling"},{"location":"file_handling/#readingwriting-text-files","text":"from-text-file - this reader reads the file line by line and forwards them to-text-file - writes the incoming strings to the specified text file","title":"Reading/writing text files"},{"location":"file_handling/#listing-files","text":"list-files - simply lists files in a directory forwards the list poll-dir - polls a directory for files to process, can move or delete them after they were processed by the specified base-reader watch-dir - uses a file-system watchdog to look for changes to files (events: created or modified) and forwards these to the base-reader, can move or delete these files after processing as well","title":"Listing files"},{"location":"file_handling/#others","text":"copy-files - copies the incoming files into the specified target directory delete-files - deletes the incoming files move-files - moves the incoming files into the specified target directory","title":"Others"},{"location":"filters/","text":"The following sections only show snippets of commands, as there are quite a number of filters available. Spectral filters # center - subtracts the column mean from the columns (batch filter) downsample - extracts every n-th wave number equi-distance - evenly spaces the wave numbers log - log-transforms the amplitudes pca - applies principal components analysis for dimensionality reduction pls1 - applies the PLS1 partial-least-squares algorithm (batch filter) rownorm (aka standard-normal-variate ) - subtracts mean and divides by standard deviation savitzky-golay and savitzky-golay2 - the Savitzky-Golay smoothing algorithm simpls - applies the SIMPLS partial-least-squares algorithm (batch filter) standardize - column-wise subtracts the column mean and divides by the column stdev (batch filter) Applying PLS1: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ pls1 \\ -l INFO \\ -n 4 \\ -r al.ext_usda.a1056_mg.kg \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata Using Savitzky-Golay: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ savitzky-golay \\ -l INFO \\ -L 3 \\ -R 5 \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata Meta-data management # metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the spectrum name via a regular expression metadata-to-placeholder - sets the specified placeholder using the data from the meta-data passing through set-metadata - sets the meta-data key/value pair as data passes through, can make use of data passing through as well split-records - adds a field to the meta-data (default: split ) of the record passing through, which can be acted on with other filters (or stored in the output) Splitting data into train/test and training center on the train batch: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ split-records \\ --split_names train test \\ --split_ratios 50 50 \\ center \\ -l INFO \\ -k split \\ --batch_order train test \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata Record management # A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-by-name - discards spectra based on their name, either using explicit names or regular expressions, e.g., excluding quality control samples max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of spectra, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream Randomizing records and outputting the first 100: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ randomize-records \\ -l INFO \\ -s 1 \\ max-records \\ -l INFO \\ -m 100 \\ to-adams \\ -l INFO \\ -o {CWD}/test/adams/output/ \\ --output_sampledata Cleaning data # Using the apply-cleaner batch filter, you can clean batches of data using any of the defined cleaners. The following applies the IQR cleaner to remove spectra that have amplitudes that are considered outliers: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ apply-cleaner \\ -l INFO \\ -c \"iqr-cl -l INFO -f 4.25\" \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata","title":"Filter usage"},{"location":"filters/#spectral-filters","text":"center - subtracts the column mean from the columns (batch filter) downsample - extracts every n-th wave number equi-distance - evenly spaces the wave numbers log - log-transforms the amplitudes pca - applies principal components analysis for dimensionality reduction pls1 - applies the PLS1 partial-least-squares algorithm (batch filter) rownorm (aka standard-normal-variate ) - subtracts mean and divides by standard deviation savitzky-golay and savitzky-golay2 - the Savitzky-Golay smoothing algorithm simpls - applies the SIMPLS partial-least-squares algorithm (batch filter) standardize - column-wise subtracts the column mean and divides by the column stdev (batch filter) Applying PLS1: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ pls1 \\ -l INFO \\ -n 4 \\ -r al.ext_usda.a1056_mg.kg \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata Using Savitzky-Golay: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ savitzky-golay \\ -l INFO \\ -L 3 \\ -R 5 \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata","title":"Spectral filters"},{"location":"filters/#meta-data-management","text":"metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the spectrum name via a regular expression metadata-to-placeholder - sets the specified placeholder using the data from the meta-data passing through set-metadata - sets the meta-data key/value pair as data passes through, can make use of data passing through as well split-records - adds a field to the meta-data (default: split ) of the record passing through, which can be acted on with other filters (or stored in the output) Splitting data into train/test and training center on the train batch: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ split-records \\ --split_names train test \\ --split_ratios 50 50 \\ center \\ -l INFO \\ -k split \\ --batch_order train test \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata","title":"Meta-data management"},{"location":"filters/#record-management","text":"A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-by-name - discards spectra based on their name, either using explicit names or regular expressions, e.g., excluding quality control samples max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of spectra, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream Randomizing records and outputting the first 100: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ randomize-records \\ -l INFO \\ -s 1 \\ max-records \\ -l INFO \\ -m 100 \\ to-adams \\ -l INFO \\ -o {CWD}/test/adams/output/ \\ --output_sampledata","title":"Record management"},{"location":"filters/#cleaning-data","text":"Using the apply-cleaner batch filter, you can clean batches of data using any of the defined cleaners. The following applies the IQR cleaner to remove spectra that have amplitudes that are considered outliers: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ apply-cleaner \\ -l INFO \\ -c \"iqr-cl -l INFO -f 4.25\" \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata","title":"Cleaning data"},{"location":"multi/","text":"Most of the time, conversion pipelines will only need to read from one source and output to a single target. However, there can be cases where data of different types need merging ( multiple inputs ) or data source of different types need to be generated for different frameworks ( multiple outputs ). To cater for these scenarios, the following two meta plugins are available: from-multi - reads from one or more sources using the specified readers to-multi - forwards the incoming data to one or more writers There is one restriction, each of the base reader/writer must be from the same data domain. Multiple inputs # Spectra # The following command reads data in Opus and ASCII XY, with the combined output being saved in ADAMS format: sdc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -r \"from-opus -l INFO -i {CWD}/input/opus/*.0\" \\ \"from-asciixy -l INFO -i {CWD}/input/asciixy/*.txt\" \\ to-adams \\ -l INFO \\ -o \"{CWD}/output\" Sample data # The following reads sample data in CSV and JSON format and outputs the records in ADAMS format: sdc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -r \"from-csv-sd -l INFO -i {CWD}/csv/input/*.csv\" \\ \"from-json-sd -l INFO -i {CWD}/json/input/*.json\" \\ to-report-sd \\ -l INFO \\ -o {CWD}/output/ Multiple outputs # Spectra # Below, the source data is in ADAMS format and will be converted to ASC and ASCII XY: sdc-convert \\ -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ to-multi \\ -l INFO \\ -w \"to-asc -l INFO -o {CWD}/output/asc\" \\ \"to-asciixy -l INFO -o {CWD}/output/asciixy\" Sample data # The following loads ADAMS sample data records and saves them in CSV and JSON format: sdc-convert \\ -l INFO \\ from-report-sd \\ -l INFO \\ -i {CWD}/adams/input/*.report \\ to-multi \\ -l INFO \\ -w \"to-csv-sd -l INFO -o {CWD}/output/{INPUT_NAMENOEXT}.csv\" \\ \"to-json-sd -l INFO -o {CWD}/output/\" NB: Since to-csv-sd is a batch writer expecting an output file rather than a directory, we have to resort to using the {INPUT_NAMENOEXT} placeholder to generate output.","title":"Multiple I/O"},{"location":"multi/#multiple-inputs","text":"","title":"Multiple inputs"},{"location":"multi/#spectra","text":"The following command reads data in Opus and ASCII XY, with the combined output being saved in ADAMS format: sdc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -r \"from-opus -l INFO -i {CWD}/input/opus/*.0\" \\ \"from-asciixy -l INFO -i {CWD}/input/asciixy/*.txt\" \\ to-adams \\ -l INFO \\ -o \"{CWD}/output\"","title":"Spectra"},{"location":"multi/#sample-data","text":"The following reads sample data in CSV and JSON format and outputs the records in ADAMS format: sdc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -r \"from-csv-sd -l INFO -i {CWD}/csv/input/*.csv\" \\ \"from-json-sd -l INFO -i {CWD}/json/input/*.json\" \\ to-report-sd \\ -l INFO \\ -o {CWD}/output/","title":"Sample data"},{"location":"multi/#multiple-outputs","text":"","title":"Multiple outputs"},{"location":"multi/#spectra_1","text":"Below, the source data is in ADAMS format and will be converted to ASC and ASCII XY: sdc-convert \\ -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ to-multi \\ -l INFO \\ -w \"to-asc -l INFO -o {CWD}/output/asc\" \\ \"to-asciixy -l INFO -o {CWD}/output/asciixy\"","title":"Spectra"},{"location":"multi/#sample-data_1","text":"The following loads ADAMS sample data records and saves them in CSV and JSON format: sdc-convert \\ -l INFO \\ from-report-sd \\ -l INFO \\ -i {CWD}/adams/input/*.report \\ to-multi \\ -l INFO \\ -w \"to-csv-sd -l INFO -o {CWD}/output/{INPUT_NAMENOEXT}.csv\" \\ \"to-json-sd -l INFO -o {CWD}/output/\" NB: Since to-csv-sd is a batch writer expecting an output file rather than a directory, we have to resort to using the {INPUT_NAMENOEXT} placeholder to generate output.","title":"Sample data"},{"location":"placeholders/","text":"Juggling longs paths in command-lines can be nightmare, which is the reason the spectral-data-converter library offers support for placeholders . Placeholders can be used to shorten paths and making command-lines easier to transfer to another environment or user. Placeholders (format {PH} ) get expanded dynamically at runtime, taking the current state into account. Placeholder types # There are different types of placeholders: System-defined ones: {HOME} - the user's home directory {CWD} - the current working directory {TMP} - the temporary directory Input-based ones, which are based on the current input file being processed: {INPUT_PATH} - the directory component of the current file {INPUT_NAMEEXT} - the name (incl ext) of the current file {INPUT_NAMENOEXT} - the name (excl ext) of the current file {INPUT_EXT} - the extension of the current file {INPUT_PARENT_PATH} - the path of the file's parent {INPUT_PARENT_NAME} - the name of the file's parent User-defined ones, which are supplied to the tool itself, e.g., via the -p/--placeholders option of the sdc-convert tool. The same script can be executed using different directories when using different placeholder setups. The format for the placeholders files is simple, one placeholder per line using placeholder=value as format. Empty lines and ones starting with # get ignored. Runtime ones, which can be set with the set-placeholder plugin. These placeholders can be based on other placeholders. The reason for this plugin is that the output of some filters may not have any directory associated with them anymore, only a file name. That renders all the input-based placeholders unusable. Using set-placeholder beforehand allows saving the input directory in another placeholder for later use. Meta-data can be used as placeholders as well using the metadata-to-placeholder plugin, which extracts a particular key from the metadata passing through and updates the specified placeholder accordingly. Examples # Relative to input # The following command places the converted data on the same level as the input directory adams in a directory called asc : sdc-convert \\ -l INFO \\ from-adams \\ -l INFO \\ -i \"/some/where/adams/*.spec\" \\ --labels a b c \\ to-asc \\ -l INFO \\ -o {INPUT_PARENT_PATH}/asc In-place predictions # When trying to convert spectra into another format and place them in the same location as the input spectra, manually copying files is rather tedious. Also, filters that get rid of the file path and only forward the file name, like savitzky-golay , invalidate the use of input-based placeholders like {INPUT_PATH} . For that reason, the set-placeholder plugin can be used to back up such placeholders in other user-defined placeholders. The following pipeline backs up {INPUT_PATH} in the new placeholder {OUTPUT_DIR} and uses that for saving the generated .asc files: sdc-convert -l INFO \\ from-adams \\ -i \"/some/where/*.spec\" \\ set-placeholder \\ -l INFO \\ -p OUTPUT_DIR \\ -v \"{INPUT_PATH}\" \\ savitzky-golay \\ to-asc \\ -l INFO \\ -o \"{OUTPUT_DIR}\"","title":"Placeholders"},{"location":"placeholders/#placeholder-types","text":"There are different types of placeholders: System-defined ones: {HOME} - the user's home directory {CWD} - the current working directory {TMP} - the temporary directory Input-based ones, which are based on the current input file being processed: {INPUT_PATH} - the directory component of the current file {INPUT_NAMEEXT} - the name (incl ext) of the current file {INPUT_NAMENOEXT} - the name (excl ext) of the current file {INPUT_EXT} - the extension of the current file {INPUT_PARENT_PATH} - the path of the file's parent {INPUT_PARENT_NAME} - the name of the file's parent User-defined ones, which are supplied to the tool itself, e.g., via the -p/--placeholders option of the sdc-convert tool. The same script can be executed using different directories when using different placeholder setups. The format for the placeholders files is simple, one placeholder per line using placeholder=value as format. Empty lines and ones starting with # get ignored. Runtime ones, which can be set with the set-placeholder plugin. These placeholders can be based on other placeholders. The reason for this plugin is that the output of some filters may not have any directory associated with them anymore, only a file name. That renders all the input-based placeholders unusable. Using set-placeholder beforehand allows saving the input directory in another placeholder for later use. Meta-data can be used as placeholders as well using the metadata-to-placeholder plugin, which extracts a particular key from the metadata passing through and updates the specified placeholder accordingly.","title":"Placeholder types"},{"location":"placeholders/#examples","text":"","title":"Examples"},{"location":"placeholders/#relative-to-input","text":"The following command places the converted data on the same level as the input directory adams in a directory called asc : sdc-convert \\ -l INFO \\ from-adams \\ -l INFO \\ -i \"/some/where/adams/*.spec\" \\ --labels a b c \\ to-asc \\ -l INFO \\ -o {INPUT_PARENT_PATH}/asc","title":"Relative to input"},{"location":"placeholders/#in-place-predictions","text":"When trying to convert spectra into another format and place them in the same location as the input spectra, manually copying files is rather tedious. Also, filters that get rid of the file path and only forward the file name, like savitzky-golay , invalidate the use of input-based placeholders like {INPUT_PATH} . For that reason, the set-placeholder plugin can be used to back up such placeholders in other user-defined placeholders. The following pipeline backs up {INPUT_PATH} in the new placeholder {OUTPUT_DIR} and uses that for saving the generated .asc files: sdc-convert -l INFO \\ from-adams \\ -i \"/some/where/*.spec\" \\ set-placeholder \\ -l INFO \\ -p OUTPUT_DIR \\ -v \"{INPUT_PATH}\" \\ savitzky-golay \\ to-asc \\ -l INFO \\ -o \"{OUTPUT_DIR}\"","title":"In-place predictions"},{"location":"pyfunc/","text":"No library can dream of offering all the required functionality. Especially for one-off tasks, it makes no sense to develop a whole new plugin library. Hence, there are the following generic plugins that allow the user to utilize custom Python functions: reader: from-pyfunc - takes a single string as input and outputs an iterable of spectrum containers filter: pyfunc-filter - takes a single spectrum container or an iterable of them as input and outputs a single container or an iterable of them writer: to-pyfunc - processes a single spectrum container or an iterable of them and an optional split name In order to use such a custom function, they must be specified in the following format (option: -f/--function ): module_name:function_name If the code below were available through module my.code , then the function specifications would be as follows: reader: my.code:pyfunc_reader filter: my.code:pyfunc_filter writer: my.code:pyfunc_writer from typing import Iterable from kasperl.api import make_list, flatten_list from sdc.api import Spectrum, Spectrum2D from wai.spectralio.adams import read # reader: generates spectrum containers from the path in ADAMS format def pyfunc_reader(path: str) -> Iterable[Spectrum2D]: for sp in read(path, options=[\"--keep-format\"]): yield Spectrum2D(source=path, spectrum=sp) # filter: simply adds a note to the meta-data def pyfunc_filter(data): result = [] for item in make_list(data): if not item.has_metadata(): meta = dict() else: meta = item.get_metadata() meta[\"note\"] = \"filtered by a python function!\" item.set_metadata(meta) result.append(item) return flatten_list(result) # writer: simply outputs name and meta-data and, if present, also the split def pyfunc_writer(data: Spectrum, split: str = None): if split is None: print(\"name: \", data.spectrum_name, \", meta:\", data.get_metadata()) else: print(\"split:\", split, \", name:\", data.spectrum_name, \", meta:\", data.get_metadata())","title":"External functions"},{"location":"sklearn/","text":"Requirements # Requires the spectral-data-converter-sklearn library. Plugins # Training a model # The following command-line loads spectra in ADAMS format, leaves only every 4th wave number, then builds a PLS regression model with three components on the al.ext_usda.a1056_mg.kg target using sklearn-fit and saves it to disk: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/train/*.spec \\ downsample \\ -n 4 \\ sklearn-fit \\ -l INFO \\ -m sklearn.cross_decomposition.PLSRegression \\ -p \"{\\\"n_components\\\": 3}\" \\ -t al.ext_usda.a1056_mg.kg \\ -o {CWD}/model/al.pkl Using a template # For more complicated setups, it can be easier to create a pickled template of the estimator/pipeline. Such a template can then be loaded via the -T/--template option of the sklearn-fit plugin. First, create the template: import pickle from sklearn.pipeline import Pipeline from sklearn.cross_decomposition import PLSRegression from sklearn.preprocessing import StandardScaler pipe = Pipeline([ ('scaler', StandardScaler()), ('pls', PLSRegression(n_components=3)) ]) with open(\"./model/al_template.pkl\", \"wb\") as fp: pickle.dump(pipe, fp) Then, make use of the template: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/train/*.spec \\ downsample \\ -n 4 \\ sklearn-fit \\ -l INFO \\ -T {CWD}/model/al_template.pkl \\ -t al.ext_usda.a1056_mg.kg \\ -o {CWD}/model/al.pkl Making predictions # Having a trained model in place, we can use it to make predictions. The sklearn-predict filter loads a pickled model from disk and then generates a prediction for each spectrum passing through, setting the value under the specified target sample data field: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/test/*.spec \\ downsample \\ -n 4 sklearn-predict \\ -l INFO \\ -m {CWD}/model/al.pkl \\ -t al.ext_usda.a1056_mg.kg \\ to-adams \\ -l INFO \\ -o {CWD}/predictions \\ --output_sampledata","title":"scikit-learn"},{"location":"sklearn/#requirements","text":"Requires the spectral-data-converter-sklearn library.","title":"Requirements"},{"location":"sklearn/#plugins","text":"","title":"Plugins"},{"location":"sklearn/#training-a-model","text":"The following command-line loads spectra in ADAMS format, leaves only every 4th wave number, then builds a PLS regression model with three components on the al.ext_usda.a1056_mg.kg target using sklearn-fit and saves it to disk: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/train/*.spec \\ downsample \\ -n 4 \\ sklearn-fit \\ -l INFO \\ -m sklearn.cross_decomposition.PLSRegression \\ -p \"{\\\"n_components\\\": 3}\" \\ -t al.ext_usda.a1056_mg.kg \\ -o {CWD}/model/al.pkl","title":"Training a model"},{"location":"sklearn/#using-a-template","text":"For more complicated setups, it can be easier to create a pickled template of the estimator/pipeline. Such a template can then be loaded via the -T/--template option of the sklearn-fit plugin. First, create the template: import pickle from sklearn.pipeline import Pipeline from sklearn.cross_decomposition import PLSRegression from sklearn.preprocessing import StandardScaler pipe = Pipeline([ ('scaler', StandardScaler()), ('pls', PLSRegression(n_components=3)) ]) with open(\"./model/al_template.pkl\", \"wb\") as fp: pickle.dump(pipe, fp) Then, make use of the template: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/train/*.spec \\ downsample \\ -n 4 \\ sklearn-fit \\ -l INFO \\ -T {CWD}/model/al_template.pkl \\ -t al.ext_usda.a1056_mg.kg \\ -o {CWD}/model/al.pkl","title":"Using a template"},{"location":"sklearn/#making-predictions","text":"Having a trained model in place, we can use it to make predictions. The sklearn-predict filter loads a pickled model from disk and then generates a prediction for each spectrum passing through, setting the value under the specified target sample data field: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/test/*.spec \\ downsample \\ -n 4 sklearn-predict \\ -l INFO \\ -m {CWD}/model/al.pkl \\ -t al.ext_usda.a1056_mg.kg \\ to-adams \\ -l INFO \\ -o {CWD}/predictions \\ --output_sampledata","title":"Making predictions"},{"location":"storage/","text":"The following pipeline components can be used for storing and retrieving data from temporary storage (i.e., a dictionary) that is available through the session object of the pipeline: delete-storage - filter that removes the name storage item from-storage - reader retrieves the named storage item set-storage - filter that updates the storage with the data passing through to-storage - writer that updates the storage with the data arriving","title":"Temporary storage"},{"location":"vis/","text":"Requirements # Requires the spectral-data-converter-vis library. Plugins # Viewing spectra # The following command loads spectra in ADAMS .spec format, down-samples them (i.e., only leaves every nth wave number) and then displays the spectra: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ downsample \\ -n 4 \\ view-spectra \\ --title ADAMS \\ --legend The resulting plot looks something like this:","title":"Visualization"},{"location":"vis/#requirements","text":"Requires the spectral-data-converter-vis library.","title":"Requirements"},{"location":"vis/#plugins","text":"","title":"Plugins"},{"location":"vis/#viewing-spectra","text":"The following command loads spectra in ADAMS .spec format, down-samples them (i.e., only leaves every nth wave number) and then displays the spectra: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ downsample \\ -n 4 \\ view-spectra \\ --title ADAMS \\ --legend The resulting plot looks something like this:","title":"Viewing spectra"},{"location":"zip/","text":"Requirements # ZIP file support comes as part of the core library (spectral-data-converter). The from-zip meta-reader and to-zip meta-writer can use any reader/writer as base reader/writer that implements the DirectReader / DirectWriter mixin, as they can handle reading from/writing to file-like objects. That applies to spectra as well as sample data readers/writers. Notes The to-zip writer is a stream writer, which means that spectra will get stored separately as individual files in the archive. This happens regardless of whether batches of data arrive. When specifying the reader/writer in the command-line, you need to use the --option=\"argument\" notion. When using --option \"argument\" , the reader/writer in the argument will get interpreted as part of the pipeline. Plugins # Reading # The following command reads spectra in ADAMS .spec format from a zip file, down-samples them (i.e., only leaves every nth wave number) and then saves them to an output directory: sdc-convert -l INFO \\ from-zip \\ -l INFO \\ -i \"./input/spec.zip\" \\ -p \"*.spec\" \\ -r=\"from-adams -l INFO\" \\ downsample \\ -n 4 \\ to-adams \\ -l INFO \\ -o ./output Sample data files can be read in the same way: sdc-convert -l INFO \\ from-zip \\ -l INFO \\ -i \"./input/sampledata.zip\" \\ -p \"*.report\" \\ -r=\"from-report-sd -l INFO\" \\ to-json-sd \\ -l INFO \\ -o ./output Writing # The command below reads spectra in ADAMS .spec format, applies row-norm (aka standard-normal-variate) to them and then stores them in DPT format in the zip file: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i \"./input/*.spec\" \\ row-norm \\ to-zip \\ -l INFO \\ -o ./output/dpt.zip \\ -w=\"to-dpt -l INFO\" We can also extract the sample data information from spectra and store just that in a zip file: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i \"./input/*.spec\" \\ spectrum-to-sampledata \\ to-zip \\ -l INFO \\ -o ./output/sampledata.zip \\ -w=\"to-json-sd -l INFO\"","title":"ZIP files"},{"location":"zip/#requirements","text":"ZIP file support comes as part of the core library (spectral-data-converter). The from-zip meta-reader and to-zip meta-writer can use any reader/writer as base reader/writer that implements the DirectReader / DirectWriter mixin, as they can handle reading from/writing to file-like objects. That applies to spectra as well as sample data readers/writers. Notes The to-zip writer is a stream writer, which means that spectra will get stored separately as individual files in the archive. This happens regardless of whether batches of data arrive. When specifying the reader/writer in the command-line, you need to use the --option=\"argument\" notion. When using --option \"argument\" , the reader/writer in the argument will get interpreted as part of the pipeline.","title":"Requirements"},{"location":"zip/#plugins","text":"","title":"Plugins"},{"location":"zip/#reading","text":"The following command reads spectra in ADAMS .spec format from a zip file, down-samples them (i.e., only leaves every nth wave number) and then saves them to an output directory: sdc-convert -l INFO \\ from-zip \\ -l INFO \\ -i \"./input/spec.zip\" \\ -p \"*.spec\" \\ -r=\"from-adams -l INFO\" \\ downsample \\ -n 4 \\ to-adams \\ -l INFO \\ -o ./output Sample data files can be read in the same way: sdc-convert -l INFO \\ from-zip \\ -l INFO \\ -i \"./input/sampledata.zip\" \\ -p \"*.report\" \\ -r=\"from-report-sd -l INFO\" \\ to-json-sd \\ -l INFO \\ -o ./output","title":"Reading"},{"location":"zip/#writing","text":"The command below reads spectra in ADAMS .spec format, applies row-norm (aka standard-normal-variate) to them and then stores them in DPT format in the zip file: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i \"./input/*.spec\" \\ row-norm \\ to-zip \\ -l INFO \\ -o ./output/dpt.zip \\ -w=\"to-dpt -l INFO\" We can also extract the sample data information from spectra and store just that in a zip file: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i \"./input/*.spec\" \\ spectrum-to-sampledata \\ to-zip \\ -l INFO \\ -o ./output/sampledata.zip \\ -w=\"to-json-sd -l INFO\"","title":"Writing"}]}