{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The spectral-data-converter library (and its dependent libraries) can be used for converting spectral data from one format into another. Please refer to the dataset formats section for more details on supported formats. But the library does not just convert data formats, you can also slot in complex filter pipelines to process/clean the data. On this website you can find examples for: Filter usage Placeholders Python functions Docker usage","title":"Home"},{"location":"docker/","text":"Below are examples for using the spectral-data-converter library via its Docker images . Interactive session # The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/spectral-data-converter:latest Conversion pipeline # The following converts an spectra in ADAMS format to ASC format: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/spectral-data-converter:latest \\ sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i /workspace/input/*.spec \\ to-asc \\ -l INFO \\ -o /workspace/output NB: The input and output directories are located below the current working directory ( pwd ).","title":"Docker usage"},{"location":"docker/#interactive-session","text":"The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/spectral-data-converter:latest","title":"Interactive session"},{"location":"docker/#conversion-pipeline","text":"The following converts an spectra in ADAMS format to ASC format: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/spectral-data-converter:latest \\ sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i /workspace/input/*.spec \\ to-asc \\ -l INFO \\ -o /workspace/output NB: The input and output directories are located below the current working directory ( pwd ).","title":"Conversion pipeline"},{"location":"filters/","text":"The following sections only show snippets of commands, as there are quite a number of filters available. Spectral filters # center - subtracts the column mean from the columns (batch filter) downsample - extracts every n-th wave number equi-distance - evenly spaces the wave numbers log - log-transforms the amplitudes pca - applies principal components analysis for dimensionality reduction pls1 - applies the PLS1 partial-least-squares algorithm (batch filter) rownorm (aka standard-normal-variate ) - subtracts mean and divides by standard deviation savitzky-golay and savitzky-golay2 - the Savitzky-Golay smoothing algorithm simpls - applies the SIMPLS partial-least-squares algorithm (batch filter) standardize - column-wise subtracts the column mean and divides by the column stdev (batch filter) Applying PLS1: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ pls1 \\ -l INFO \\ -n 4 \\ -r al.ext_usda.a1056_mg.kg \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata Using Savitzky-Golay: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ savitzky-golay \\ -l INFO \\ -L 3 \\ -R 5 \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata Meta-data management # metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the spectrum name via a regular expression metadata-to-placeholder - turns meta-data into placeholders, which can be used for redirecting output of writers split-records - adds the field split to the meta-data of the record passing through, which can be acted on with other filters (or stored in the output) Splitting data into train/test and training center on the train batch: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ split-records \\ --split_names train test \\ --split_ratios 50 50 \\ center \\ -l INFO \\ -k split \\ --batch_order train test \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata Record management # A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-by-name - discards spectra based on their name, either using explicit names or regular expressions, e.g., excluding quality control samples max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of spectra, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream Randomizing records and outputting the first 100: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ randomize-records \\ -l INFO \\ -s 1 \\ max-records \\ -l INFO \\ -m 100 \\ to-adams \\ -l INFO \\ -o {CWD}/test/adams/output/ \\ --output_sampledata","title":"Filter usage"},{"location":"filters/#spectral-filters","text":"center - subtracts the column mean from the columns (batch filter) downsample - extracts every n-th wave number equi-distance - evenly spaces the wave numbers log - log-transforms the amplitudes pca - applies principal components analysis for dimensionality reduction pls1 - applies the PLS1 partial-least-squares algorithm (batch filter) rownorm (aka standard-normal-variate ) - subtracts mean and divides by standard deviation savitzky-golay and savitzky-golay2 - the Savitzky-Golay smoothing algorithm simpls - applies the SIMPLS partial-least-squares algorithm (batch filter) standardize - column-wise subtracts the column mean and divides by the column stdev (batch filter) Applying PLS1: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ pls1 \\ -l INFO \\ -n 4 \\ -r al.ext_usda.a1056_mg.kg \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata Using Savitzky-Golay: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ savitzky-golay \\ -l INFO \\ -L 3 \\ -R 5 \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata","title":"Spectral filters"},{"location":"filters/#meta-data-management","text":"metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the spectrum name via a regular expression metadata-to-placeholder - turns meta-data into placeholders, which can be used for redirecting output of writers split-records - adds the field split to the meta-data of the record passing through, which can be acted on with other filters (or stored in the output) Splitting data into train/test and training center on the train batch: sdc-convert -l INFO -b \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ split-records \\ --split_names train test \\ --split_ratios 50 50 \\ center \\ -l INFO \\ -k split \\ --batch_order train test \\ to-adams \\ -l INFO \\ -o {CWD}/output/ \\ --output_sampledata","title":"Meta-data management"},{"location":"filters/#record-management","text":"A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-by-name - discards spectra based on their name, either using explicit names or regular expressions, e.g., excluding quality control samples max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of spectra, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream Randomizing records and outputting the first 100: sdc-convert -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ randomize-records \\ -l INFO \\ -s 1 \\ max-records \\ -l INFO \\ -m 100 \\ to-adams \\ -l INFO \\ -o {CWD}/test/adams/output/ \\ --output_sampledata","title":"Record management"},{"location":"multi/","text":"Most of the time, conversion pipelines will only need to read from one source and output to a single target. However, there can be cases where data of different types need merging ( multiple inputs ) or data source of different types need to be generated for different frameworks ( multiple outputs ). To cater for these scenarios, the following two meta plugins are available: from-multi - reads from one or more sources using the specified readers to-multi - forwards the incoming data to one or more writers There is one restriction, each of the base reader/writer must be from the same data domain. Multiple inputs # The following command reads data in Opus and ASCII XY, with the combined output being saved in ADAMS format: sdc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -r \"from-opus -l INFO -i {CWD}/input/opus/*.0\" \\ \"from-asciixy -l INFO -i {CWD}/input/asciixy/*.txt\" \\ to-adams \\ -l INFO \\ -o \"{CWD}/output\" Multiple outputs # Below, the source data is in ADAMS format and will be converted to ASC and ASCII XY: sdc-convert \\ -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ to-multi \\ -l INFO \\ -w \"to-asc -l INFO -o {CWD}/output/asc\" \\ \"to-asciixy -l INFO -o {CWD}/output/asciixy\" Sub-pipelines # With the tee meta-filter, it is possible to filter the spectra coming through with a separate sub-pipeline. That allows converting the incoming data into multiple output formats with their own preprocessing. The following command loads the ADAMS annotations and saves them in ASC and ASCII XY format in one command, but one with centered and the other with log-transformed data: sdc-convert \\ -l INFO \\ -b \\ from-adams \\ -l INFO \\ -i \"./adams/*.spec\" \\ tee \\ -f \"center to-asc -l INFO -o ./tee-asc/\" \\ tee \\ -f \"standardize to-asciixy -l INFO -o ./tee-asciixy/\"","title":"Multiple I/O"},{"location":"multi/#multiple-inputs","text":"The following command reads data in Opus and ASCII XY, with the combined output being saved in ADAMS format: sdc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -r \"from-opus -l INFO -i {CWD}/input/opus/*.0\" \\ \"from-asciixy -l INFO -i {CWD}/input/asciixy/*.txt\" \\ to-adams \\ -l INFO \\ -o \"{CWD}/output\"","title":"Multiple inputs"},{"location":"multi/#multiple-outputs","text":"Below, the source data is in ADAMS format and will be converted to ASC and ASCII XY: sdc-convert \\ -l INFO \\ from-adams \\ -l INFO \\ -i {CWD}/input/*.spec \\ to-multi \\ -l INFO \\ -w \"to-asc -l INFO -o {CWD}/output/asc\" \\ \"to-asciixy -l INFO -o {CWD}/output/asciixy\"","title":"Multiple outputs"},{"location":"multi/#sub-pipelines","text":"With the tee meta-filter, it is possible to filter the spectra coming through with a separate sub-pipeline. That allows converting the incoming data into multiple output formats with their own preprocessing. The following command loads the ADAMS annotations and saves them in ASC and ASCII XY format in one command, but one with centered and the other with log-transformed data: sdc-convert \\ -l INFO \\ -b \\ from-adams \\ -l INFO \\ -i \"./adams/*.spec\" \\ tee \\ -f \"center to-asc -l INFO -o ./tee-asc/\" \\ tee \\ -f \"standardize to-asciixy -l INFO -o ./tee-asciixy/\"","title":"Sub-pipelines"},{"location":"placeholders/","text":"Juggling longs paths in command-lines can be nightmare, which is the reason the spectral-data-converter library offers support for placeholders . Placeholders can be used to shorten paths and making command-lines easier to transfer to another environment or user. Placeholders (format {PH} ) get expanded dynamically at runtime, taking the current state into account. Placeholder types # There are different types of placeholders: System-defined ones: {HOME} - the user's home directory {CWD} - the current working directory {TMP} - the temporary directory Input-based ones, which are based on the current input file being processed: {INPUT_PATH} - the directory component of the current file {INPUT_NAMEEXT} - the name (incl ext) of the current file {INPUT_NAMENOEXT} - the name (excl ext) of the current file {INPUT_EXT} - the extension of the current file {INPUT_PARENT_PATH} - the path of the file's parent {INPUT_PARENT_NAME} - the name of the file's parent User-defined ones, which are supplied to the tool itself, e.g., via the -p/--placeholders option of the sdc-convert tool. The same script can be executed using different directories when using different placeholder setups. The format for the placeholders files is simple, one placeholder per line using placeholder=value as format. Empty lines and ones starting with # get ignored. Runtime ones, which can be set with the set-placeholder plugin. These placeholders can be based on other placeholders. The reason for this plugin is that the output of some filters may not have any directory associated with them anymore, only a file name. That renders all the input-based placeholders unusable. Using set-placeholder beforehand allows saving the input directory in another placeholder for later use. Meta-data can be used as placeholders as well using the metadata-to-placeholder plugin, which extracts a particular key from the metadata passing through and updates the specified placeholder accordingly. Examples # Relative to input # The following command places the converted data on the same level as the input directory adams in a directory called asc : sdc-convert \\ -l INFO \\ from-adams \\ -l INFO \\ -i \"/some/where/adams/*.spec\" \\ --labels a b c \\ to-asc \\ -l INFO \\ -o {INPUT_PARENT_PATH}/asc In-place predictions # When trying to convert spectra into another format and place them in the same location as the input spectra, manually copying files is rather tedious. Also, filters that get rid of the file path and only forward the file name, like savitzky-golay , invalidate the use of input-based placeholders like {INPUT_PATH} . For that reason, the set-placeholder plugin can be used to back up such placeholders in other user-defined placeholders. The following pipeline backs up {INPUT_PATH} in the new placeholder {OUTPUT_DIR} and uses that for saving the generated .asc files: sdc-convert -l INFO \\ from-adams \\ -i \"/some/where/*.spec\" \\ set-placeholder \\ -l INFO \\ -p OUTPUT_DIR \\ -v \"{INPUT_PATH}\" \\ savitzky-golay \\ to-asc \\ -l INFO \\ -o \"{OUTPUT_DIR}\"","title":"Placeholders"},{"location":"placeholders/#placeholder-types","text":"There are different types of placeholders: System-defined ones: {HOME} - the user's home directory {CWD} - the current working directory {TMP} - the temporary directory Input-based ones, which are based on the current input file being processed: {INPUT_PATH} - the directory component of the current file {INPUT_NAMEEXT} - the name (incl ext) of the current file {INPUT_NAMENOEXT} - the name (excl ext) of the current file {INPUT_EXT} - the extension of the current file {INPUT_PARENT_PATH} - the path of the file's parent {INPUT_PARENT_NAME} - the name of the file's parent User-defined ones, which are supplied to the tool itself, e.g., via the -p/--placeholders option of the sdc-convert tool. The same script can be executed using different directories when using different placeholder setups. The format for the placeholders files is simple, one placeholder per line using placeholder=value as format. Empty lines and ones starting with # get ignored. Runtime ones, which can be set with the set-placeholder plugin. These placeholders can be based on other placeholders. The reason for this plugin is that the output of some filters may not have any directory associated with them anymore, only a file name. That renders all the input-based placeholders unusable. Using set-placeholder beforehand allows saving the input directory in another placeholder for later use. Meta-data can be used as placeholders as well using the metadata-to-placeholder plugin, which extracts a particular key from the metadata passing through and updates the specified placeholder accordingly.","title":"Placeholder types"},{"location":"placeholders/#examples","text":"","title":"Examples"},{"location":"placeholders/#relative-to-input","text":"The following command places the converted data on the same level as the input directory adams in a directory called asc : sdc-convert \\ -l INFO \\ from-adams \\ -l INFO \\ -i \"/some/where/adams/*.spec\" \\ --labels a b c \\ to-asc \\ -l INFO \\ -o {INPUT_PARENT_PATH}/asc","title":"Relative to input"},{"location":"placeholders/#in-place-predictions","text":"When trying to convert spectra into another format and place them in the same location as the input spectra, manually copying files is rather tedious. Also, filters that get rid of the file path and only forward the file name, like savitzky-golay , invalidate the use of input-based placeholders like {INPUT_PATH} . For that reason, the set-placeholder plugin can be used to back up such placeholders in other user-defined placeholders. The following pipeline backs up {INPUT_PATH} in the new placeholder {OUTPUT_DIR} and uses that for saving the generated .asc files: sdc-convert -l INFO \\ from-adams \\ -i \"/some/where/*.spec\" \\ set-placeholder \\ -l INFO \\ -p OUTPUT_DIR \\ -v \"{INPUT_PATH}\" \\ savitzky-golay \\ to-asc \\ -l INFO \\ -o \"{OUTPUT_DIR}\"","title":"In-place predictions"},{"location":"pyfunc/","text":"No library can dream of offering all the required functionality. Especially for one-off tasks, it makes no sense to develop a whole new plugin library. Hence, there are the following generic plugins that allow the user to utilize custom Python functions: reader: from-pyfunc - takes a single string as input and outputs an iterable of spectrum containers filter: pyfunc-filter - takes a single spectrum container or an iterable of them as input and outputs a single container or an iterable of them writer: to-pyfunc - processes a single spectrum container or an iterable of them and an optional split name In order to use such a custom function, they must be specified in the following format (option: -f/--function ): module_name:function_name If the code below were available through module my.code , then the function specifications would be as follows: reader: my.code:pyfunc_reader filter: my.code:pyfunc_filter writer: my.code:pyfunc_writer from typing import Iterable from sdc.api import Spectrum, Spectrum2D, make_list, flatten_list from wai.spectralio.adams import read # reader: generates spectrum containers from the path in ADAMS format def pyfunc_reader(path: str) -> Iterable[Spectrum2D]: for sp in read(path, options=[\"--keep-format\"]): yield Spectrum2D(source=path, spectrum=sp) # filter: simply adds a note to the meta-data def pyfunc_filter(data): result = [] for item in make_list(data): if not item.has_metadata(): meta = dict() else: meta = item.get_metadata() meta[\"note\"] = \"filtered by a python function!\" item.set_metadata(meta) result.append(item) return flatten_list(result) # writer: simply outputs name and meta-data and, if present, also the split def pyfunc_writer(data: Spectrum, split: str = None): if split is None: print(\"name: \", data.spectrum_name, \", meta:\", data.get_metadata()) else: print(\"split:\", split, \", name:\", data.spectrum_name, \", meta:\", data.get_metadata())","title":"External functions"}]}